# Day 977: Parameter Shift Rules & Gradients

## Schedule Overview

| Block | Time | Duration | Activity |
|-------|------|----------|----------|
| Morning | 9:00 AM - 12:30 PM | 3.5 hours | Theory: Gradient Computation Methods |
| Afternoon | 2:00 PM - 4:30 PM | 2.5 hours | Problem Solving |
| Evening | 7:00 PM - 8:00 PM | 1 hour | Computational Lab |

**Total Study Time:** 7 hours

---

## Learning Objectives

By the end of Day 977, you will be able to:

1. Derive the parameter shift rule for quantum gradients
2. Apply the rule to multi-parameter and generalized gates
3. Understand the relationship between parameter shift and finite differences
4. Compute the quantum Fisher information matrix
5. Implement natural gradient optimization
6. Analyze shot noise effects on gradient estimation

---

## Core Content

### 1. The Gradient Challenge

For variational quantum algorithms, we need:
$$\frac{\partial}{\partial \theta_i} \langle \psi(\boldsymbol{\theta}) | H | \psi(\boldsymbol{\theta}) \rangle$$

**Why is this hard?**
- $|\psi(\theta)\rangle$ is a quantum state, not directly accessible
- Classical finite differences require small $\epsilon$, problematic with shot noise
- Need method compatible with quantum measurement

**The Solution:** Parameter shift rule gives *exact* gradients from circuit evaluations.

---

### 2. Derivation of the Parameter Shift Rule

Consider a parameterized gate generated by Hermitian operator $G$:
$$U(\theta) = e^{-i\theta G/2}$$

where $G$ has eigenvalues $\pm 1$ (e.g., Pauli operators).

**The circuit:**
$$|\psi(\theta)\rangle = U(\theta) |\phi\rangle = e^{-i\theta G/2} |\phi\rangle$$

**The cost function:**
$$C(\theta) = \langle \psi(\theta) | H | \psi(\theta) \rangle = \langle \phi | e^{i\theta G/2} H e^{-i\theta G/2} | \phi \rangle$$

**Taking the derivative:**
$$\frac{\partial C}{\partial \theta} = \frac{i}{2} \langle \phi | e^{i\theta G/2} [G, H] e^{-i\theta G/2} | \phi \rangle$$

**Key insight:** For $G^2 = I$ (like Pauli operators):
$$e^{-i\theta G/2} = \cos(\theta/2) I - i\sin(\theta/2) G$$

This gives:
$$\boxed{\frac{\partial C}{\partial \theta} = \frac{1}{2}\left[C\left(\theta + \frac{\pi}{2}\right) - C\left(\theta - \frac{\pi}{2}\right)\right]}$$

**The Parameter Shift Rule!**

---

### 3. General Form and Conditions

**Standard Parameter Shift (Two-Term):**

For gates $U(\theta) = e^{-i\theta G}$ where $G$ has two distinct eigenvalues $\pm r$:

$$\frac{\partial C}{\partial \theta} = r \left[C\left(\theta + \frac{\pi}{4r}\right) - C\left(\theta - \frac{\pi}{4r}\right)\right]$$

For Pauli generators ($r = 1/2$): shift by $\pm\pi/2$

**Common Gates and Their Shifts:**

| Gate | Generator | Shift |
|------|-----------|-------|
| $R_X(\theta)$ | $X/2$ | $\pm\pi/2$ |
| $R_Y(\theta)$ | $Y/2$ | $\pm\pi/2$ |
| $R_Z(\theta)$ | $Z/2$ | $\pm\pi/2$ |
| $R_{XX}(\theta)$ | $XX/2$ | $\pm\pi/2$ |
| $CR_X(\theta)$ | $(I-Z)X/4$ | $\pm\pi/2$ |

**Multi-Eigenvalue Generators:**

For generators with more eigenvalues, use generalized parameter shift:
$$\frac{\partial C}{\partial \theta} = \sum_{k} c_k C(\theta + s_k)$$

where $\{c_k, s_k\}$ are determined by the eigenvalue spectrum.

---

### 4. Comparison with Finite Differences

**Finite Difference Method:**
$$\frac{\partial C}{\partial \theta} \approx \frac{C(\theta + \epsilon) - C(\theta - \epsilon)}{2\epsilon}$$

**Comparison:**

| Aspect | Parameter Shift | Finite Difference |
|--------|-----------------|-------------------|
| Bias | Unbiased (exact) | Biased $O(\epsilon^2)$ |
| Circuit runs | 2 per parameter | 2 per parameter |
| Step size choice | Fixed ($\pi/2$) | Must tune $\epsilon$ |
| Shot noise | Amplified at small $\epsilon$ | Trade-off with bias |

**Why Parameter Shift Wins:**

At small $\epsilon$, finite difference variance blows up:
$$\text{Var}\left[\frac{C(\theta+\epsilon) - C(\theta-\epsilon)}{2\epsilon}\right] \sim \frac{\sigma^2}{\epsilon^2}$$

Parameter shift has fixed, optimal variance.

---

### 5. Gradient Estimation Variance

**Single Gradient Component:**

With $M$ shots for each circuit evaluation:
$$\text{Var}[\hat{g}] = \frac{1}{4}\left(\text{Var}[\hat{C}_+] + \text{Var}[\hat{C}_-]\right) \approx \frac{1}{2M}$$

assuming unit variance observables.

**Total Gradient (p parameters):**

Need $2p$ circuit evaluations. Total variance in gradient norm:
$$\text{Var}[\|\nabla C\|^2] \sim \frac{p}{M}$$

**Optimal Shot Allocation:**

For correlated parameters, allocate more shots to higher-variance components. Adaptive strategies can reduce total cost.

---

### 6. Higher-Order Derivatives

**Hessian (Second Derivatives):**

Apply parameter shift twice:
$$\frac{\partial^2 C}{\partial \theta_i \partial \theta_j} = \frac{1}{4}\left[C_{++} - C_{+-} - C_{-+} + C_{--}\right]$$

where subscripts indicate shifts in $\theta_i$ and $\theta_j$.

**Cost:** 4 circuit evaluations per Hessian element, $O(p^2)$ total.

**Applications:**
- Newton's method optimization
- Curvature analysis
- Landscape characterization

---

### 7. Quantum Fisher Information and Natural Gradients

**Quantum Fisher Information Matrix (QFIM):**

$$g_{ij} = \text{Re}\left[\langle \partial_i \psi | \partial_j \psi \rangle - \langle \partial_i \psi | \psi \rangle \langle \psi | \partial_j \psi \rangle\right]$$

This is the Fubini-Study metric on the parameter manifold.

**Natural Gradient:**

Instead of $\theta \leftarrow \theta - \eta \nabla C$, use:
$$\theta \leftarrow \theta - \eta \, g^{-1} \nabla C$$

This accounts for the geometry of the parameter space.

**Why Natural Gradient?**
- Invariant to parameterization
- Faster convergence in curved landscapes
- Avoids getting stuck in flat regions

**Computing QFIM on Hardware:**

Using parameter shift:
$$g_{ij} = \frac{1}{2}\text{Re}\left[\langle \psi | G_i G_j | \psi \rangle - \langle \psi | G_i | \psi \rangle \langle \psi | G_j | \psi \rangle\right]$$

---

### 8. Stochastic Gradient Methods

**Stochastic Gradient Descent (SGD):**
- Estimate gradient with finite shots
- Update: $\theta \leftarrow \theta - \eta \hat{\nabla} C$

**SPSA (Simultaneous Perturbation Stochastic Approximation):**
- Only 2 circuit evaluations regardless of $p$
- Random direction gradient estimate
- Higher variance but much cheaper

$$\hat{g}_i = \frac{C(\theta + \epsilon \Delta) - C(\theta - \epsilon \Delta)}{2\epsilon \Delta_i}$$

where $\Delta$ is a random direction (e.g., Rademacher).

**Adam for Quantum:**
- Momentum: smooth noisy gradients
- Adaptive learning rate per parameter
- Popular in QML applications

---

## Practical Applications

### Gradient-Based VQE Optimization

Standard VQE uses gradient-free optimizers (COBYLA, SPSA) due to historical measurement cost concerns. With parameter shift:

**Gradient-Based VQE Workflow:**
1. Initialize parameters $\theta^{(0)}$
2. For each iteration:
   a. Compute $C(\theta)$ and $\nabla C(\theta)$ via parameter shift
   b. Update $\theta$ using Adam/SGD/Natural gradient
3. Converge when $\|\nabla C\| < \epsilon$

**Advantages:**
- Faster convergence (fewer iterations)
- Better final accuracy
- Works well with error mitigation

---

## Worked Examples

### Example 1: Single-Qubit Gradient

**Problem:** Compute $\frac{\partial}{\partial \theta} \langle Z \rangle$ for the circuit $R_Y(\theta)|0\rangle$.

**Solution:**

The state is:
$$|\psi(\theta)\rangle = R_Y(\theta)|0\rangle = \cos(\theta/2)|0\rangle + \sin(\theta/2)|1\rangle$$

The expectation value:
$$C(\theta) = \langle Z \rangle = \cos^2(\theta/2) - \sin^2(\theta/2) = \cos\theta$$

**Analytical gradient:**
$$\frac{\partial C}{\partial \theta} = -\sin\theta$$

**Parameter shift:**
$$\frac{\partial C}{\partial \theta} = \frac{1}{2}\left[\cos(\theta + \pi/2) - \cos(\theta - \pi/2)\right]$$
$$= \frac{1}{2}\left[-\sin\theta - \sin\theta\right] = -\sin\theta \quad \checkmark$$

---

### Example 2: Two-Qubit Gradient

**Problem:** Compute $\frac{\partial}{\partial \theta} \langle Z_0 Z_1 \rangle$ for $R_{XX}(\theta)|00\rangle$.

**Solution:**

The $R_{XX}$ gate:
$$R_{XX}(\theta) = e^{-i\theta X_0 X_1/2} = \cos(\theta/2)I - i\sin(\theta/2)X_0X_1$$

Acting on $|00\rangle$:
$$|\psi\rangle = \cos(\theta/2)|00\rangle - i\sin(\theta/2)|11\rangle$$

The expectation:
$$\langle Z_0Z_1 \rangle = \cos^2(\theta/2)(+1) + \sin^2(\theta/2)(+1) = 1$$

Wait, this is constant! The gradient should be zero.

**Verification with parameter shift:**
$$\frac{\partial C}{\partial \theta} = \frac{1}{2}[1 - 1] = 0 \quad \checkmark$$

**Physical insight:** $Z_0Z_1$ commutes with $R_{XX}$, so the expectation is conserved.

---

### Example 3: Shot Noise Analysis

**Problem:** Estimate the number of shots needed to determine a gradient to precision $\delta = 0.01$.

**Solution:**

The gradient variance from parameter shift:
$$\text{Var}[\hat{g}] \approx \frac{1}{2M}$$

for a bounded observable with variance $O(1)$.

For precision $\delta$, we need:
$$\sqrt{\text{Var}[\hat{g}]} \leq \delta$$
$$\frac{1}{\sqrt{2M}} \leq 0.01$$
$$M \geq \frac{1}{2 \times 0.01^2} = 5000 \text{ shots}$$

Per circuit. Total for one gradient: $2 \times 5000 = 10,000$ shots.

For $p = 20$ parameters: $2 \times 20 \times 5000 = 200,000$ shots per gradient step.

**Conclusion:** Gradient estimation is expensive! Motivates SPSA and other methods.

---

## Practice Problems

### Level 1: Direct Application

1. Apply the parameter shift rule to compute $\frac{\partial}{\partial \theta}\langle Y \rangle$ for $R_X(\theta)|0\rangle$.

2. How many circuit evaluations are needed to compute the full gradient for a 10-parameter ansatz using parameter shift?

3. For the gate $R_Z(\theta)$, verify that the parameter shift formula gives the correct gradient for $\langle X \rangle$.

### Level 2: Intermediate

4. Derive the parameter shift rule for the controlled-RY gate with generator $G = (I - Z_0)Y_1/4$.

5. Compute the Hessian element $\frac{\partial^2 C}{\partial \theta_1 \partial \theta_2}$ for $C = \langle Z \rangle$ with circuit $R_Y(\theta_1)R_X(\theta_2)|0\rangle$.

6. For SPSA with perturbation size $\epsilon = 0.1$ and variance in $C$ of $\sigma^2 = 0.1$, estimate the variance in the gradient estimate.

### Level 3: Challenging

7. Derive the parameter shift rule for a gate with generator eigenvalues $\{-1, 0, 0, 1\}$ (like the $XX + YY$ gate on 2 qubits).

8. Prove that the quantum Fisher information matrix $g_{ij}$ is positive semi-definite.

9. **Research:** How can analytic gradients be combined with error mitigation? What is the impact on gradient variance?

---

## Computational Lab

### Objective
Implement parameter shift gradients and compare optimization methods.

```python
"""
Day 977 Computational Lab: Parameter Shift Rules & Gradients
Advanced Variational Methods - Week 140
"""

import numpy as np
import pennylane as qml
from pennylane import numpy as pnp
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# =============================================================================
# Part 1: Parameter Shift Rule Verification
# =============================================================================

print("=" * 70)
print("Part 1: Parameter Shift Rule Verification")
print("=" * 70)

dev = qml.device('default.qubit', wires=1)

@qml.qnode(dev)
def simple_circuit(theta):
    qml.RY(theta, wires=0)
    return qml.expval(qml.PauliZ(0))

# Analytical gradient
def analytical_gradient(theta):
    return -np.sin(theta)

# Parameter shift gradient
def parameter_shift_gradient(circuit, theta, shift=np.pi/2):
    return 0.5 * (circuit(theta + shift) - circuit(theta - shift))

# Finite difference gradient
def finite_diff_gradient(circuit, theta, epsilon=0.01):
    return (circuit(theta + epsilon) - circuit(theta - epsilon)) / (2 * epsilon)

# Compare at multiple points
thetas = np.linspace(0, 2*np.pi, 20)
grad_analytical = [analytical_gradient(t) for t in thetas]
grad_param_shift = [parameter_shift_gradient(simple_circuit, t) for t in thetas]
grad_finite_diff = [finite_diff_gradient(simple_circuit, t) for t in thetas]

print("\nGradient Comparison at theta = pi/4:")
t = np.pi/4
print(f"  Analytical:       {analytical_gradient(t):.6f}")
print(f"  Parameter shift:  {parameter_shift_gradient(simple_circuit, t):.6f}")
print(f"  Finite diff:      {finite_diff_gradient(simple_circuit, t):.6f}")

# =============================================================================
# Part 2: Multi-Parameter Gradient
# =============================================================================

print("\n" + "=" * 70)
print("Part 2: Multi-Parameter Gradient")
print("=" * 70)

n_qubits = 3
dev_multi = qml.device('default.qubit', wires=n_qubits)

# Create a simple Hamiltonian
H = qml.Hamiltonian(
    [1.0, 0.5, 0.5],
    [qml.PauliZ(0), qml.PauliZ(1), qml.PauliX(0) @ qml.PauliX(1)]
)

@qml.qnode(dev_multi)
def variational_circuit(params):
    for i in range(n_qubits):
        qml.RY(params[i], wires=i)
    qml.CNOT(wires=[0, 1])
    qml.CNOT(wires=[1, 2])
    for i in range(n_qubits):
        qml.RY(params[n_qubits + i], wires=i)
    return qml.expval(H)

def compute_full_gradient_param_shift(circuit, params):
    """Compute gradient for all parameters using parameter shift."""
    gradients = []
    for i in range(len(params)):
        params_plus = params.copy()
        params_minus = params.copy()
        params_plus[i] += np.pi/2
        params_minus[i] -= np.pi/2

        grad_i = 0.5 * (circuit(params_plus) - circuit(params_minus))
        gradients.append(grad_i)
    return np.array(gradients)

# Test gradient computation
params = np.random.uniform(0, 2*np.pi, 2*n_qubits)
gradient = compute_full_gradient_param_shift(variational_circuit, params)
print(f"\nParameters: {params}")
print(f"Gradient: {gradient}")
print(f"Gradient norm: {np.linalg.norm(gradient):.6f}")

# Verify with PennyLane's autodiff
@qml.qnode(dev_multi)
def circuit_autodiff(params):
    for i in range(n_qubits):
        qml.RY(params[i], wires=i)
    qml.CNOT(wires=[0, 1])
    qml.CNOT(wires=[1, 2])
    for i in range(n_qubits):
        qml.RY(params[n_qubits + i], wires=i)
    return qml.expval(H)

grad_autodiff = qml.grad(circuit_autodiff)(pnp.array(params, requires_grad=True))
print(f"\nPennyLane autodiff gradient: {grad_autodiff}")
print(f"Max difference: {np.max(np.abs(gradient - grad_autodiff)):.2e}")

# =============================================================================
# Part 3: Shot Noise Effects
# =============================================================================

print("\n" + "=" * 70)
print("Part 3: Shot Noise Effects on Gradients")
print("=" * 70)

# Device with finite shots
shot_values = [100, 500, 1000, 5000, 10000]
gradient_variances = []

params_fixed = np.array([0.5, 0.3, 0.7, 0.2, 0.9, 0.4])
true_gradient = compute_full_gradient_param_shift(variational_circuit, params_fixed)

for n_shots in shot_values:
    dev_shots = qml.device('default.qubit', wires=n_qubits, shots=n_shots)

    @qml.qnode(dev_shots)
    def circuit_shots(params):
        for i in range(n_qubits):
            qml.RY(params[i], wires=i)
        qml.CNOT(wires=[0, 1])
        qml.CNOT(wires=[1, 2])
        for i in range(n_qubits):
            qml.RY(params[n_qubits + i], wires=i)
        return qml.expval(H)

    # Sample gradients multiple times
    gradient_samples = []
    for _ in range(20):
        grad = compute_full_gradient_param_shift(circuit_shots, params_fixed)
        gradient_samples.append(grad)

    gradient_samples = np.array(gradient_samples)
    variance = np.mean(np.var(gradient_samples, axis=0))
    gradient_variances.append(variance)
    print(f"  Shots = {n_shots:5d}: Variance = {variance:.6f}")

# =============================================================================
# Part 4: Gradient-Based vs Gradient-Free Optimization
# =============================================================================

print("\n" + "=" * 70)
print("Part 4: Optimization Method Comparison")
print("=" * 70)

# Create a VQE problem
@qml.qnode(dev_multi)
def vqe_circuit(params):
    for i in range(n_qubits):
        qml.RY(params[i], wires=i)
    qml.CNOT(wires=[0, 1])
    qml.CNOT(wires=[1, 2])
    for i in range(n_qubits):
        qml.RY(params[n_qubits + i], wires=i)
    qml.CNOT(wires=[0, 1])
    for i in range(n_qubits):
        qml.RZ(params[2*n_qubits + i], wires=i)
    return qml.expval(H)

n_params = 3 * n_qubits

# Track optimization history
def optimize_with_history(optimizer_name, max_iters=100):
    x0 = np.random.uniform(0, np.pi, n_params)
    history = [vqe_circuit(x0)]
    params = x0.copy()

    if optimizer_name == 'GD':
        # Gradient descent with parameter shift
        eta = 0.1
        for _ in range(max_iters):
            grad = compute_full_gradient_param_shift(vqe_circuit, params)
            params = params - eta * grad
            history.append(vqe_circuit(params))

    elif optimizer_name == 'Adam':
        # Adam optimizer
        eta = 0.1
        beta1, beta2 = 0.9, 0.999
        eps = 1e-8
        m = np.zeros(n_params)
        v = np.zeros(n_params)

        for t in range(1, max_iters + 1):
            grad = compute_full_gradient_param_shift(vqe_circuit, params)
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad**2
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            params = params - eta * m_hat / (np.sqrt(v_hat) + eps)
            history.append(vqe_circuit(params))

    elif optimizer_name == 'COBYLA':
        # Gradient-free
        result = minimize(vqe_circuit, x0, method='COBYLA',
                         options={'maxiter': max_iters},
                         callback=lambda x: history.append(vqe_circuit(x)))

    return history

print("\nOptimizing with different methods...")
history_gd = optimize_with_history('GD', max_iters=50)
history_adam = optimize_with_history('Adam', max_iters=50)
history_cobyla = optimize_with_history('COBYLA', max_iters=50)

print(f"  GD final energy:     {history_gd[-1]:.6f}")
print(f"  Adam final energy:   {history_adam[-1]:.6f}")
print(f"  COBYLA final energy: {history_cobyla[-1]:.6f}")

# =============================================================================
# Part 5: Quantum Fisher Information
# =============================================================================

print("\n" + "=" * 70)
print("Part 5: Quantum Fisher Information Matrix")
print("=" * 70)

@qml.qnode(dev_multi)
def state_circuit(params):
    for i in range(n_qubits):
        qml.RY(params[i], wires=i)
    qml.CNOT(wires=[0, 1])
    return qml.state()

def compute_qfim(params, n_params):
    """
    Compute quantum Fisher information matrix.
    g_ij = Re[<di psi|dj psi> - <di psi|psi><psi|dj psi>]
    """
    eps = 0.001
    qfim = np.zeros((n_params, n_params))

    psi_0 = state_circuit(params)

    for i in range(n_params):
        params_i_plus = params.copy()
        params_i_minus = params.copy()
        params_i_plus[i] += eps
        params_i_minus[i] -= eps

        # Approximate derivative via finite difference
        psi_i_plus = state_circuit(params_i_plus)
        psi_i_minus = state_circuit(params_i_minus)
        dpsi_i = (psi_i_plus - psi_i_minus) / (2 * eps)

        for j in range(i, n_params):
            params_j_plus = params.copy()
            params_j_minus = params.copy()
            params_j_plus[j] += eps
            params_j_minus[j] -= eps

            psi_j_plus = state_circuit(params_j_plus)
            psi_j_minus = state_circuit(params_j_minus)
            dpsi_j = (psi_j_plus - psi_j_minus) / (2 * eps)

            # g_ij = Re[<dpsi_i|dpsi_j> - <dpsi_i|psi><psi|dpsi_j>]
            term1 = np.vdot(dpsi_i, dpsi_j)
            term2 = np.vdot(dpsi_i, psi_0) * np.vdot(psi_0, dpsi_j)

            qfim[i, j] = np.real(term1 - term2)
            qfim[j, i] = qfim[i, j]

    return qfim

# Compute QFIM for simple case
simple_params = np.array([0.5, 0.3, 0.7])
qfim = compute_qfim(simple_params, 3)
print("\nQuantum Fisher Information Matrix:")
print(qfim)
print(f"\nEigenvalues: {np.linalg.eigvalsh(qfim)}")

# =============================================================================
# Part 6: Visualization
# =============================================================================

print("\n" + "=" * 70)
print("Part 6: Visualization")
print("=" * 70)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Gradient comparison
ax1 = axes[0, 0]
ax1.plot(thetas, grad_analytical, 'k-', linewidth=2, label='Analytical')
ax1.plot(thetas, grad_param_shift, 'ro', markersize=6, label='Param. Shift')
ax1.plot(thetas, grad_finite_diff, 'b^', markersize=4, alpha=0.7, label='Finite Diff.')
ax1.set_xlabel('Theta')
ax1.set_ylabel('Gradient')
ax1.set_title('Gradient Methods Comparison')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Shot noise effect
ax2 = axes[0, 1]
ax2.loglog(shot_values, gradient_variances, 'o-', markersize=8)
expected = [1/(2*s) for s in shot_values]
ax2.loglog(shot_values, expected, 'k--', label=r'$1/(2M)$ theory')
ax2.set_xlabel('Number of Shots')
ax2.set_ylabel('Gradient Variance')
ax2.set_title('Shot Noise Effect on Gradient')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Optimization comparison
ax3 = axes[1, 0]
ax3.plot(history_gd, 'b-', label='Gradient Descent')
ax3.plot(history_adam, 'r-', label='Adam')
ax3.plot(history_cobyla, 'g-', label='COBYLA')
ax3.set_xlabel('Iteration')
ax3.set_ylabel('Energy')
ax3.set_title('Optimization Convergence')
ax3.legend()
ax3.grid(True, alpha=0.3)

# QFIM visualization
ax4 = axes[1, 1]
im = ax4.imshow(qfim, cmap='RdBu_r', vmin=-np.max(np.abs(qfim)),
                vmax=np.max(np.abs(qfim)))
ax4.set_title('Quantum Fisher Information Matrix')
ax4.set_xlabel('Parameter j')
ax4.set_ylabel('Parameter i')
plt.colorbar(im, ax=ax4)

plt.tight_layout()
plt.savefig('day_977_gradients.png', dpi=150, bbox_inches='tight')
plt.show()
print("\nFigure saved as 'day_977_gradients.png'")

print("\n" + "=" * 70)
print("Lab Complete!")
print("=" * 70)
```

---

## Summary

### Key Formulas

| Concept | Formula |
|---------|---------|
| Parameter shift rule | $\frac{\partial C}{\partial \theta} = \frac{1}{2}[C(\theta + \pi/2) - C(\theta - \pi/2)]$ |
| General shift | $\frac{\partial C}{\partial \theta} = r[C(\theta + \frac{\pi}{4r}) - C(\theta - \frac{\pi}{4r})]$ |
| Gradient variance | $\text{Var}[\hat{g}] \approx \frac{1}{2M}$ |
| QFIM element | $g_{ij} = \text{Re}[\langle \partial_i \psi \| \partial_j \psi \rangle - \langle \partial_i \psi \| \psi \rangle \langle \psi \| \partial_j \psi \rangle]$ |
| Natural gradient | $\Delta \theta = -\eta \, g^{-1} \nabla C$ |

### Main Takeaways

1. **Parameter shift gives exact gradients** without finite difference bias
2. **Two circuit evaluations per parameter** is the cost
3. **Shot noise affects gradient precision** as $O(1/\sqrt{M})$
4. **Natural gradient** uses quantum geometry for better optimization
5. **SPSA and other methods** trade accuracy for fewer circuit calls
6. **Gradient-based optimization** often outperforms gradient-free for smooth landscapes

---

## Daily Checklist

- [ ] Derive parameter shift for RX, RY, RZ gates
- [ ] Understand shot noise effects on gradients
- [ ] Work through all three examples
- [ ] Complete Level 1 practice problems
- [ ] Attempt at least one Level 2 problem
- [ ] Run and modify the computational lab
- [ ] Compare Adam vs COBYLA convergence rates

---

## Preview: Day 978

Tomorrow we tackle the critical challenge of **barren plateaus**â€”the phenomenon where gradients vanish exponentially with system size, making optimization intractable. We'll explore causes and mitigation strategies.

---

*"The gradient tells you where to go; the Fisher metric tells you how far."*
--- Geometric optimization wisdom

---

**Next:** [Day_978_Friday.md](Day_978_Friday.md) - Barren Plateau Mitigation
